{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ae13317",
      "metadata": {
        "id": "0ae13317"
      },
      "source": [
        "## What is Retrieval-Augmented Generation (RAG)?\n",
        "Retrieval-Augmented Generation (RAG) is an AI architecture that combines two key techniques **retrieval and generation**.\n",
        "\n",
        "1) First, the system retrieves relevant information from external sources such as databases, document collections, or the web.\n",
        "2) Then, it uses this retrieved information to enhance the generation of responses or outputs.\n",
        "\n",
        "By integrating retrieval with generation, RAG allows AI models to produce more accurate, and contextually relevant answers, even when the required information is not part of the model’s original training data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d268e1",
      "metadata": {
        "id": "70d268e1"
      },
      "source": [
        "## The High-Level Architecture of RAG (Semanctic Search)\n",
        "\n",
        "1. **Data Ingestion Pipeline:** Converts documents into vector embeddings and stores them in a vector database for efficient semantic lookup.\n",
        "2. **Retrieval Pipeline:** Transforms the user query into an embedding and retrieves semantically similar documents from the vector store.\n",
        "3. **Generation Pipeline:** Combines the retrieved context with the query to generate a relevant, context-aware response.\n",
        "\n",
        "<img src=\"https://i.ibb.co/wFT8HRbb/rag.png\"\n",
        "     alt=\"RAG Architecture\"\n",
        "     style=\"max-width: 90%; height: auto; display: block;\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ff51ec",
      "metadata": {
        "id": "21ff51ec"
      },
      "source": [
        "### Instslling Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721226f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "721226f3",
        "outputId": "8bd90485-e3ef-4c67-fa31-2a600d0cc6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (4.1.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.56.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.45.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install langchain-core langchain-text-splitters langchain-google-genai python-dotenv langchain_community wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480c6c12",
      "metadata": {
        "id": "480c6c12"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY=\"your google api key\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033fd7d9",
      "metadata": {
        "id": "033fd7d9"
      },
      "source": [
        "### Download the source data from Wikipedia\n",
        "\n",
        "Source: https://en.wikipedia.org/wiki/Retrieval-augmented_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af761893",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af761893",
        "outputId": "5f33095a-c93a-4d06-db8b-fd61f7d16b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "\n",
        "# Download the source data from wikipedia\n",
        "loader = WikipediaLoader(query=\"Retrieval-augmented generation\", load_max_docs=1, doc_content_chars_max=20000)\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef189bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ef189bd",
        "outputId": "b4780c1f-dc42-475e-d72d-e886c7da1547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Retrieval-augmented generation', 'summary': 'Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.', 'source': 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation'}, page_content='Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.\\n\\n\\n== RAG and LLM limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without prompt stuffing, the LLM\\'s input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information\" (\"augmentation\"). IBM states that \"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant\".\\n\\n\\n=== RAG key stages ===\\n\\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.\\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user\\'s original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.\\nFinally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.\\n\\n\\n== Improvements ==\\nImprovements to the basic process above can be applied at different stages in the RAG flow. \\n\\n\\n=== Encoder ===\\nThese methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).  \\n\\nPerformance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.\\nAccuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.\\nHybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.\\nOther retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.\\n\\n\\n=== Retriever-centric methods ===\\nThese methods aim to enhance the quality of document retrieval in vector databases:\\n\\nPre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents.\\nSupervised retriever optimization aligns retrieval probabilities with the generator model’s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the retriever’s selections and the model’s likelihoods to refine retrieval.\\nReranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.\\n\\n\\n=== Language model ===\\n\\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.  \\nIt has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.\\n\\n\\n=== Chunking ===\\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\\n\\nThree types of chunking strategies are:\\n\\nFixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.\\nSyntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.\\nFile format-based chunking. Certain file types have natural chunks built in, and it\\'s best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method.\\n\\n\\n=== Hybrid search ===\\nSometimes vector database searches can miss key facts needed to answer a user\\'s question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.\\n\\n\\n=== Evaluation and benchmarks ===\\nRAG systems are commonly evaluated using benchmarks designed to test retrievability, retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.\\n\\n\\n== Challenges ==\\nRAG does not prevent hallucinations in LLMs. According to Ars Technica, \"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\"\\nWhile RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.\\nRAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.\\n\\n\\n== References ==')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d72f16",
      "metadata": {
        "id": "76d72f16"
      },
      "outputs": [],
      "source": [
        "source = \"\"\n",
        "for doc in data:\n",
        "    source += doc.page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b3af07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "d0b3af07",
        "outputId": "14c37904-e197-4f69-eeaa-0249a0d70b72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper.\\n\\n\\n== RAG and LLM limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without prompt stuffing, the LLM\\'s input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information\" (\"augmentation\"). IBM states that \"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant\".\\n\\n\\n=== RAG key stages ===\\n\\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.\\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user\\'s original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.\\nFinally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.\\n\\n\\n== Improvements ==\\nImprovements to the basic process above can be applied at different stages in the RAG flow. \\n\\n\\n=== Encoder ===\\nThese methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).  \\n\\nPerformance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.\\nAccuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.\\nHybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.\\nOther retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.\\n\\n\\n=== Retriever-centric methods ===\\nThese methods aim to enhance the quality of document retrieval in vector databases:\\n\\nPre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents.\\nSupervised retriever optimization aligns retrieval probabilities with the generator model’s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the retriever’s selections and the model’s likelihoods to refine retrieval.\\nReranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.\\n\\n\\n=== Language model ===\\n\\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.  \\nIt has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.\\n\\n\\n=== Chunking ===\\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\\n\\nThree types of chunking strategies are:\\n\\nFixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.\\nSyntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.\\nFile format-based chunking. Certain file types have natural chunks built in, and it\\'s best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method.\\n\\n\\n=== Hybrid search ===\\nSometimes vector database searches can miss key facts needed to answer a user\\'s question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.\\n\\n\\n=== Evaluation and benchmarks ===\\nRAG systems are commonly evaluated using benchmarks designed to test retrievability, retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.\\n\\n\\n== Challenges ==\\nRAG does not prevent hallucinations in LLMs. According to Ars Technica, \"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\"\\nWhile RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.\\nRAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.\\n\\n\\n== References =='"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "source"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e898409",
      "metadata": {
        "id": "4e898409"
      },
      "source": [
        "### Text chunking (Recursive Text Chunking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c330b8a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c330b8a2",
        "outputId": "f1c7f6d7-d83b-4669-cf14-966c401fa03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunk :  105\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information from',\n",
              " 'incorporate new information from external data sources. With RAG, LLMs do not respond to user queries until they refer to a specified set of',\n",
              " \"they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use\",\n",
              " 'data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps',\n",
              " 'data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.',\n",
              " 'RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike LLMs that rely on static',\n",
              " 'Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica,',\n",
              " 'According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document',\n",
              " 'a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused',\n",
              " \"hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking\",\n",
              " 'cases to lawyers that are looking for citations to support their arguments.',\n",
              " 'RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs',\n",
              " 'gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as',\n",
              " 'provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.',\n",
              " 'The term RAG was first introduced in a 2020 research paper.',\n",
              " '== RAG and LLM limitations ==',\n",
              " 'LLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect',\n",
              " 'Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s',\n",
              " 'billion decline in the company’s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can',\n",
              " 'problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT',\n",
              " 'they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim',\n",
              " 'United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein',\n",
              " 'rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title,',\n",
              " 'the context of the title, generating a false statement.',\n",
              " 'LLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without prompt stuffing, the LLM\\'s',\n",
              " \"Without prompt stuffing, the LLM's input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide\",\n",
              " 'is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to',\n",
              " 'in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.',\n",
              " '== Process ==',\n",
              " 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to',\n",
              " 'mechanism that allows models to access and utilize additional data beyond their original training set. Ars Technica notes that \"when new information',\n",
              " 'notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external',\n",
              " 'is to augment the model’s external knowledge base with the updated information\" (\"augmentation\"). IBM states that \"in the generative phase, the LLM',\n",
              " '\"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging',\n",
              " 'data to synthesize an engaging answer tailored to the user in that instant\".',\n",
              " '=== RAG key stages ===',\n",
              " 'Typically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used',\n",
              " 'vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings',\n",
              " 'graphs). These embeddings are then stored in a vector database to allow for document retrieval.',\n",
              " 'Given a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This',\n",
              " 'be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.',\n",
              " \"The model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of\",\n",
              " 'Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains',\n",
              " 'queries into multiple domains and using memory and self-improvement to learn from previous retrievals.',\n",
              " 'Finally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such',\n",
              " 'steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.',\n",
              " '== Improvements ==\\nImprovements to the basic process above can be applied at different stages in the RAG flow.',\n",
              " '=== Encoder ===',\n",
              " 'These methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically',\n",
              " 'identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer',\n",
              " 'are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).',\n",
              " 'Performance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest',\n",
              " 'scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.',\n",
              " 'Accuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document',\n",
              " 'This helps refine document ranking and improve search relevance.',\n",
              " 'Hybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational',\n",
              " 'advantage of the computational efficiency of sparse dot products over dense vector operations.',\n",
              " 'Other retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations,',\n",
              " 'combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.',\n",
              " '=== Retriever-centric methods ===\\nThese methods aim to enhance the quality of document retrieval in vector databases:',\n",
              " 'Pre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text',\n",
              " 'patterns by predicting masked text within documents.',\n",
              " 'Supervised retriever optimization aligns retrieval probabilities with the generator model’s likelihood distribution. This involves retrieving the',\n",
              " 'This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the',\n",
              " 'KL divergence between the retriever’s selections and the model’s likelihoods to refine retrieval.',\n",
              " 'Reranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.',\n",
              " '=== Language model ===',\n",
              " 'By redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger',\n",
              " 'perplexity as its much larger counterparts. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the',\n",
              " 'cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus',\n",
              " 'training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model',\n",
              " 'The redesigned language model is shown here.',\n",
              " 'It has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and',\n",
              " 'version is called Retro++ and includes in-context RAG.',\n",
              " '=== Chunking ===\\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.',\n",
              " 'Three types of chunking strategies are:',\n",
              " 'Fixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.',\n",
              " 'Syntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.',\n",
              " \"File format-based chunking. Certain file types have natural chunks built in, and it's best to respect them. For example, code files are best chunked\",\n",
              " 'code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact.',\n",
              " 'encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this',\n",
              " 'or Langchain can assist with this method.',\n",
              " '=== Hybrid search ===',\n",
              " \"Sometimes vector database searches can miss key facts needed to answer a user's question. One way to mitigate this is to do a traditional text\",\n",
              " 'this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the',\n",
              " 'the vector search, and feed the combined hybrid text into the language model for generation.',\n",
              " '=== Evaluation and benchmarks ===',\n",
              " 'RAG systems are commonly evaluated using benchmarks designed to test retrievability, retrieval accuracy and generative quality. Popular datasets',\n",
              " 'quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for',\n",
              " 'Natural Questions or Google QA for open-domain QA.',\n",
              " '== Challenges ==',\n",
              " 'RAG does not prevent hallucinations in LLMs. According to Ars Technica, \"It is not a direct solution because the LLM can still hallucinate around the',\n",
              " 'can still hallucinate around the source material in its response.\"',\n",
              " 'While RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the',\n",
              " 'is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when',\n",
              " 'may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate',\n",
              " 'training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the',\n",
              " 'can arise when the model lacks the ability to assess its own knowledge limitations.',\n",
              " 'RAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements',\n",
              " 'an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with',\n",
              " 'Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this',\n",
              " 'The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and',\n",
              " 'responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG',\n",
              " 'these issues occur because RAG systems may misinterpret the data they retrieve.',\n",
              " '== References ==']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 150, chunk_overlap = 35)\n",
        "\n",
        "chunks = splitter.split_text(source)\n",
        "\n",
        "print(\"Number of chunk : \", len(chunks))\n",
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10167a0e",
      "metadata": {
        "id": "10167a0e"
      },
      "source": [
        "### Initialize Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14aa3349",
      "metadata": {
        "id": "14aa3349"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedder = GoogleGenerativeAIEmbeddings(api_key=GOOGLE_API_KEY, model=\"models/text-embedding-004\", task_type=\"RETRIEVAL_DOCUMENT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723aefcc",
      "metadata": {
        "id": "723aefcc"
      },
      "source": [
        "### Initialize the Vector DB and Auto Generation of Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec632240",
      "metadata": {
        "id": "ec632240"
      },
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vectorstore = InMemoryVectorStore.from_texts(texts=chunks, embedding=embedder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2477ff4a",
      "metadata": {
        "id": "2477ff4a"
      },
      "source": [
        "### User Query and Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eccf744c",
      "metadata": {
        "id": "eccf744c"
      },
      "outputs": [],
      "source": [
        "user_query = \"What is the purpose of Inverse Cloze Task\"\n",
        "\n",
        "# Embed the user query to perfom vector similarity search\n",
        "query_embeddings = embedder.embed_query(user_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea11f1e",
      "metadata": {
        "id": "5ea11f1e"
      },
      "source": [
        "### Retriving Relevant Chunks from Vector DataBase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77bef47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b77bef47",
        "outputId": "5f3c7b9b-848a-4bd7-f5c4-dbdf60852e60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='b1e550f5-9a58-49f5-983f-89edd598339f', metadata={}, page_content='Pre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text'),\n",
              " Document(id='b0577605-8bbf-4076-ae8f-75ea20a23963', metadata={}, page_content='This involves retrieving the top-k vectors for a given prompt, scoring the generated response’s perplexity, and minimizing KL divergence between the'),\n",
              " Document(id='ea592674-31e4-463d-befb-22210ddec0e0', metadata={}, page_content='patterns by predicting masked text within documents.'),\n",
              " Document(id='8d3c04d4-db3a-4283-9e97-a57978cc3806', metadata={}, page_content='=== Language model ==='),\n",
              " Document(id='0028a2e1-f538-4776-b7f8-fab7e033c602', metadata={}, page_content='in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "retriver = vectorstore.similarity_search_by_vector(embedding=query_embeddings, k=5)\n",
        "retriver"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0584a5e3",
      "metadata": {
        "id": "0584a5e3"
      },
      "source": [
        "### Initialize Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2034376",
      "metadata": {
        "id": "f2034376"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "\n",
        "llm = GoogleGenerativeAI(api_key=GOOGLE_API_KEY, model=\"gemini-2.5-flash\", temperature=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903d174f",
      "metadata": {
        "id": "903d174f"
      },
      "source": [
        "### System Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c75da8",
      "metadata": {
        "id": "c1c75da8"
      },
      "outputs": [],
      "source": [
        "context = \"\\n\".join([doc.page_content for doc in retriver])\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a knowledgeable assistant.\n",
        "Answer the question using ONLY the context below.\n",
        "If the answer is not present, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{user_query}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d80847c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d80847c",
        "outputId": "4d3fee42-00b7-47a5-a569-3c92e9bd4669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:  The Inverse Cloze Task (ICT) is a technique that helps the model learn retrieval patterns by predicting masked text within documents.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "print(\"Response: \", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4cf0095",
      "metadata": {
        "id": "f4cf0095"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}